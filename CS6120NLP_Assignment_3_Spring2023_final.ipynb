{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNMrNlx_plWE"
      },
      "source": [
        "# **CS 6120: Natural Language Processing - Prof. Ahmad Uzair** \n",
        "\n",
        "### **Assignment 3: n-gram Language Models, Word Sense disambiguation(LSA using SVD), LSTM**\n",
        "\n",
        "### **Total points: 100**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2lfntjwAMLl"
      },
      "source": [
        "# Q1. Latent Semantic Analysis (35 Points) \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51a5UGlaAMLl"
      },
      "source": [
        " - A. Singular Value Decomposition (SVD) based distributed representation of text and documents. You can use python libraries for matrix decomposition (scipy). To demonstrate your work, use the example dataset (Table 2) of \"R. A. Harshman (1990). Indexing by latent semantic analysis. Journal of the American society for information science\". (10 Points)\n",
        "\n",
        " - B. Visualize (2-D) the documents and terms using library of your choice. (10 Points)\n",
        "\n",
        " - C. Implement a function that converts a query string to distributed representation and retrieves relevent documents. Visualize the the results as shown in Fig 1 of the paper. (10 Points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9BatoveAMLm"
      },
      "source": [
        "## <CENTER>Task-1 (10 Points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aj820sOAAMLm"
      },
      "source": [
        "### Input data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "dJ_MSpFDAMLn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cca3f31b-63ba-48bb-df44-920dd3e46219"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "#nltk.download()\n",
        "import re\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stopword = nltk.corpus.stopwords.words('english')\n",
        "#nltk.download('punkt')\n",
        "#nltk.download('wordnet')\n",
        "\n",
        "#Dataset\n",
        "c1 = 'Computer vision is a field of artificial intelligence that focuses on enabling computers to interpret and understand visual information from the world.'\n",
        "c2 = 'One of the most prominent applications of computer vision is in autonomous vehicles, where it helps the vehicle \"see\" and make decisions based on its surroundings.'\n",
        "c3 = 'Computer vision is also used in facial recognition technology, which has become controversial due to concerns over privacy and potential misuse.'\n",
        "c4 = 'In the medical field, computer vision is used to assist doctors in diagnosing diseases and analyzing medical images such as x-rays and MRIs.'\n",
        "c5 = 'Computer vision is also used in security and surveillance systems, where it can detect and recognize suspicious activities or individuals.'\n",
        "m1 = 'Cybersecurity refers to the practices and technologies used to protect computer systems, networks, and data from unauthorized access, use, disclosure, disruption, modification, or destruction.'\n",
        "m2 = 'One of the most important applications of cybersecurity is in safeguarding sensitive data and personal information, such as financial data or healthcare records.'\n",
        "m3 = 'Cybersecurity is also essential in protecting critical infrastructure, such as power grids and transportation systems, from cyber attacks that could cause significant disruptions.'\n",
        "m4 = 'In the healthcare industry, cybersecurity is used to protect medical devices and prevent unauthorized access to patient data.'\n",
        "documents = [c1, c2, c3, c4, c5, m1, m2, m3, m4]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9-a9HfZAMLo"
      },
      "source": [
        "### Perform preprocessing of documents\n",
        "\n",
        "In the below cell remove punctuations and lowercase the message"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "cXUWU0cZAMLo"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "string.punctuation # checking punctuations\n",
        "\n",
        "# TASK CELL\n",
        "\n",
        "def preprocess(message):\n",
        "    '''\n",
        "    Input:\n",
        "        message: a string containing a message.\n",
        "    Output:\n",
        "        preprocessed_message_list: a list of words containing the processed message. \n",
        "\n",
        "    '''\n",
        "    preprocessed_message_list = \"\".join([char for char in message if char not in string.punctuation]).lower().split(\" \")\n",
        "   \n",
        "\n",
        "    return preprocessed_message_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-oKwALnAMLo"
      },
      "source": [
        "### Verify preprocessed data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "rtHfRaHtAMLo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7fc4087-a31c-426e-bf3c-aa257c561337"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['computer', 'vision', 'is', 'a', 'field', 'of', 'artificial', 'intelligence', 'that', 'focuses', 'on', 'enabling', 'computers', 'to', 'interpret', 'and', 'understand', 'visual', 'information', 'from', 'the', 'world']\n",
            "['one', 'of', 'the', 'most', 'prominent', 'applications', 'of', 'computer', 'vision', 'is', 'in', 'autonomous', 'vehicles', 'where', 'it', 'helps', 'the', 'vehicle', 'see', 'and', 'make', 'decisions', 'based', 'on', 'its', 'surroundings']\n",
            "['computer', 'vision', 'is', 'also', 'used', 'in', 'facial', 'recognition', 'technology', 'which', 'has', 'become', 'controversial', 'due', 'to', 'concerns', 'over', 'privacy', 'and', 'potential', 'misuse']\n",
            "['in', 'the', 'medical', 'field', 'computer', 'vision', 'is', 'used', 'to', 'assist', 'doctors', 'in', 'diagnosing', 'diseases', 'and', 'analyzing', 'medical', 'images', 'such', 'as', 'xrays', 'and', 'mris']\n",
            "['computer', 'vision', 'is', 'also', 'used', 'in', 'security', 'and', 'surveillance', 'systems', 'where', 'it', 'can', 'detect', 'and', 'recognize', 'suspicious', 'activities', 'or', 'individuals']\n",
            "['cybersecurity', 'refers', 'to', 'the', 'practices', 'and', 'technologies', 'used', 'to', 'protect', 'computer', 'systems', 'networks', 'and', 'data', 'from', 'unauthorized', 'access', 'use', 'disclosure', 'disruption', 'modification', 'or', 'destruction']\n",
            "['one', 'of', 'the', 'most', 'important', 'applications', 'of', 'cybersecurity', 'is', 'in', 'safeguarding', 'sensitive', 'data', 'and', 'personal', 'information', 'such', 'as', 'financial', 'data', 'or', 'healthcare', 'records']\n",
            "['cybersecurity', 'is', 'also', 'essential', 'in', 'protecting', 'critical', 'infrastructure', 'such', 'as', 'power', 'grids', 'and', 'transportation', 'systems', 'from', 'cyber', 'attacks', 'that', 'could', 'cause', 'significant', 'disruptions']\n",
            "['in', 'the', 'healthcare', 'industry', 'cybersecurity', 'is', 'used', 'to', 'protect', 'medical', 'devices', 'and', 'prevent', 'unauthorized', 'access', 'to', 'patient', 'data']\n"
          ]
        }
      ],
      "source": [
        "for sent in documents:\n",
        "    print(preprocess(sent))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CxjNhwfAMLp"
      },
      "source": [
        "##### Expected Output\n",
        "```CPP\n",
        "['computer', 'vision', 'is', 'a', 'field', 'of', 'artificial', 'intelligence', 'that', 'focuses', 'on', 'enabling', 'computers', 'to', 'interpret', 'and', 'understand', 'visual', 'information', 'from', 'the', 'world']\n",
        "['one', 'of', 'the', 'most', 'prominent', 'applications', 'of', 'computer', 'vision', 'is', 'in', 'autonomous', 'vehicles', 'where', 'it', 'helps', 'the', 'vehicle', '\"see\"', 'and', 'make', 'decisions', 'based', 'on', 'its', 'surroundings']\n",
        "['computer', 'vision', 'is', 'also', 'used', 'in', 'facial', 'recognition', 'technology', 'which', 'has', 'become', 'controversial', 'due', 'to', 'concerns', 'over', 'privacy', 'and', 'potential', 'misuse']\n",
        "['in', 'the', 'medical', 'field', 'computer', 'vision', 'is', 'used', 'to', 'assist', 'doctors', 'in', 'diagnosing', 'diseases', 'and', 'analyzing', 'medical', 'images', 'such', 'as', 'x', 'rays', 'and', 'MRIs']\n",
        "['computer', 'vision', 'is', 'also', 'used', 'in', 'security', 'and', 'surveillance', 'systems', 'where', 'it', 'can', 'detect', 'and', 'recognize', 'suspicious', 'activities', 'or', 'individuals']\n",
        "['Cybersecurity', 'refers', 'to', 'the', 'practices', 'and', 'technologies', 'used', 'to', 'protect', 'computer', 'systems', 'networks', 'and', 'data', 'from', 'unauthorized', 'access', 'use', 'disclosure', 'disruption', 'modification', 'or', 'destruction']\n",
        "['one', 'of', 'the', 'most', 'important', 'applications', 'of', 'cybersecurity', 'is', 'in', 'safeguarding', 'sensitive', 'data', 'and', 'personal', 'information', 'such', 'as', 'financial', 'data', 'or', 'healthcare', 'records']\n",
        "['Cybersecurity', 'is', 'also', 'essential', 'in', 'protecting', 'critical', 'infrastructure', 'such', 'as', 'power', 'grids', 'and', 'transportation', 'systems', 'from', 'cyber', 'attacks', 'that', 'could', 'cause', 'significant', 'disruptions']\n",
        "['in', 'the', 'healthcare', 'industry', 'cybersecurity', 'is', 'used', 'to', 'protect', 'medical', 'devices', 'and', 'prevent', 'unauthorized', 'access', 'to', 'patient', 'data']\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WDV3e-4AMLp"
      },
      "source": [
        "### Assign names to document names\n",
        "In the below cell create a list of document names. It will be later used to visualize documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "SxujZEbrAMLp"
      },
      "outputs": [],
      "source": [
        "def createDocName(documents):\n",
        "     '''\n",
        "    Input:\n",
        "        documents: list of documents.\n",
        "    Output:\n",
        "        doc_names: a list of document names. \n",
        "\n",
        "    '''\n",
        "     names = [1] * len(documents)\n",
        "     vars = re.findall(r'\\\\n\\w\\d', str(globals()));\n",
        "     \n",
        "     for i in range(0, len(documents)):\n",
        "          name = vars[i]\n",
        "          names[i] = name[2:]\n",
        "          \n",
        "     return names\n",
        "     \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ovu9s_XCAMLp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36a13e3c-f534-435a-d4c1-8fef9dff0727"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['c1', 'c2', 'c3', 'c4', 'c5', 'm1', 'm2', 'm3', 'm4']\n"
          ]
        }
      ],
      "source": [
        "docName = createDocName(documents)\n",
        "print(docName)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-hbWnH6AMLp"
      },
      "source": [
        "##### Expected Output\n",
        "``` CPP\n",
        "['c1', 'c2', 'c3', 'c4', 'c5', 'm1', 'm2', 'm3', 'm4']\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwDGBYP0AMLq"
      },
      "source": [
        "### Words to Index mapping\n",
        "Retrieve words from documents and create map of word and associate index to it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "tPPwoLDRAMLq"
      },
      "outputs": [],
      "source": [
        "def build_word_to_ix(documents, stopwords=None):\n",
        "     '''\n",
        "    Input:\n",
        "        documents: list of documents\n",
        "        stopwords: list of stopwords\n",
        "    Output:\n",
        "        doc_names: map of words and associated index. Make sure to remove words which occur in less than 2 documents \n",
        "    '''\n",
        "     \n",
        "     documents_zip = zip(documents, range(0,len(documents)))\n",
        "     word_to_ind = {}\n",
        "     indices = {}\n",
        "\n",
        "     for doc, j in documents_zip:\n",
        "         words = preprocess(doc)\n",
        "         n = len(words)\n",
        "\n",
        "         for i in range(0, len(words)):\n",
        "            if(words[i] in stopwords):\n",
        "               continue\n",
        "            \n",
        "            whereSeen = word_to_ind.get(words[i])\n",
        "            if(whereSeen == None ):\n",
        "             word_to_ind[words[i]] = str(j)\n",
        "\n",
        "            elif(whereSeen[len(whereSeen)-1] != str(j)):\n",
        "\n",
        "             word_to_ind[words[i]] = \"\".join([whereSeen, str(j)])\n",
        "\n",
        "     count = 0\n",
        "     for key, value in word_to_ind.items():\n",
        "        if(len(value) >= 2):\n",
        "           indices[key] = count\n",
        "           count+=1\n",
        "        \n",
        "            \n",
        "            \n",
        "    \n",
        "     return indices\n",
        "     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "AJOXHCtPAMLq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4032f41-d69a-4b74-a601-a201a0265ece"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'computer': 0, 'vision': 1, 'field': 2, 'information': 3, 'one': 4, 'applications': 5, 'also': 6, 'used': 7, 'medical': 8, 'systems': 9, 'cybersecurity': 10, 'protect': 11, 'data': 12, 'unauthorized': 13, 'access': 14, 'healthcare': 15}\n"
          ]
        }
      ],
      "source": [
        "word_to_ix = build_word_to_ix(documents, stopword)\n",
        "print(word_to_ix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZlo0SxRAMLq"
      },
      "source": [
        "##### Expected Output\n",
        "Note: the index value for each token could be different in your implementation \n",
        "```CPP\n",
        "{'field': 0, 'vision': 1, 'information': 2, 'computer': 3, 'one': 4, 'applications': 5, 'also': 6, 'used': 7, 'medical': 8, 'systems': 9, 'data': 10, 'unauthorized': 11, 'access': 12, 'cybersecurity': 13, 'protect': 14, 'healthcare': 15}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpNH0AoFAMLq"
      },
      "source": [
        "### Document-Terms count matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "pjbiP8koAMLq"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "def build_td_matrix(documents, word_to_ix):\n",
        "    '''\n",
        "    Input:\n",
        "        documents: list of documents.\n",
        "        word_to_ix: {word, index} map\n",
        "    Output:\n",
        "        td_matrix: matrix of count of words in documents, each row \n",
        "            represent a word and each column represent a document\n",
        "\n",
        "    '''\n",
        "    vectorizer = CountVectorizer(vocabulary=word_to_ix);\n",
        "    td_matrix = vectorizer.fit_transform(documents).toarray()\n",
        "\n",
        "    return td_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "Hw12VPjTAMLq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e2cba17-6372-4f0d-8ee7-ab1a8692911f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(9, 16)\n"
          ]
        }
      ],
      "source": [
        "X = build_td_matrix(documents, word_to_ix)\n",
        "print(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTdOUAMnAMLq"
      },
      "source": [
        "##### Expected Output\n",
        "Note: the order of rows could be different in your implementation as it is based on the indexing of the tokens done in build_word_to_ix\n",
        "```CPP\n",
        "[[1. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
        " [1. 1. 1. 1. 1. 0. 0. 0. 0.]\n",
        " [1. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
        " [1. 1. 1. 1. 1. 1. 0. 0. 0.]\n",
        " [0. 1. 0. 0. 0. 0. 1. 0. 0.]\n",
        " [0. 1. 0. 0. 0. 0. 1. 0. 0.]\n",
        " [0. 0. 1. 0. 1. 0. 0. 1. 0.]\n",
        " [0. 0. 1. 1. 1. 1. 0. 0. 1.]\n",
        " [0. 0. 0. 2. 0. 0. 0. 0. 1.]\n",
        " [0. 0. 0. 0. 1. 1. 0. 1. 0.]\n",
        " [0. 0. 0. 0. 0. 1. 2. 0. 1.]\n",
        " [0. 0. 0. 0. 0. 1. 0. 0. 1.]\n",
        " [0. 0. 0. 0. 0. 1. 0. 0. 1.]\n",
        " [0. 0. 0. 0. 0. 1. 1. 1. 1.]\n",
        " [0. 0. 0. 0. 0. 1. 0. 0. 1.]\n",
        " [0. 0. 0. 0. 0. 0. 1. 0. 1.]]\n",
        " ```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeBW0dr7AMLr"
      },
      "source": [
        "### Singular Value Decomposition\n",
        "Perform singular value decomposition of count matrix into term singular vector matrix, singular value matrix and document singular vector matrix\n",
        "- To perform the singular value decompostion please check tutorial:\n",
        "https://numpy.org/doc/stable/reference/generated/numpy.linalg.svd.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "FXCpmScoAMLr"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "def svd(documents, word_to_ix, rank):\n",
        "    '''\n",
        "    Input:\n",
        "        documents: list of documents.\n",
        "        word_to_ix: {word, index} map\n",
        "        rank: number of colums/rows to retain in decomposed matrix\n",
        "    Output:\n",
        "        Uk: term singular vector matrix\n",
        "        Sk: singular value matrix\n",
        "        Vk_t: transpose of document singular vector matrix\n",
        "    '''\n",
        "    \n",
        "    Uk, Sk, Vk_t = np.linalg.svd(build_td_matrix(documents, word_to_ix))\n",
        "    return Uk[:, :rank], Sk[:rank], Vk_t[:, :rank]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "EBsAlKrTAMLr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "568c49b3-36bc-4897-9e4b-d7e382c87bdc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-0.20485755  0.28230559]\n",
            " [-0.2018926   0.18797401]\n",
            " [-0.27781813  0.33979025]\n",
            " [-0.3848213   0.42056737]\n",
            " [-0.32282413  0.34698447]\n",
            " [-0.49850192 -0.22485343]\n",
            " [-0.3270043  -0.52403905]\n",
            " [-0.14631016 -0.03570198]\n",
            " [-0.4592868  -0.38181953]]\n",
            "[4.63682644 3.4660758 ]\n",
            "[[-0.40776071 -0.30025142]\n",
            " [ 0.39028813  0.45516076]\n",
            " [ 0.12622415  0.2552902 ]\n",
            " [ 0.11067417  0.01081877]\n",
            " [ 0.38986849  0.05886294]\n",
            " [-0.04176591  0.07983177]\n",
            " [-0.07859318  0.12648487]\n",
            " [ 0.14063179 -0.28970048]\n",
            " [ 0.27558757 -0.25154893]\n",
            " [ 0.1776376  -0.33434321]\n",
            " [-0.26218017  0.35352935]\n",
            " [-0.17369044  0.08702693]\n",
            " [ 0.1121266   0.41265184]\n",
            " [-0.17369044  0.08702693]\n",
            " [-0.17369044  0.08702693]\n",
            " [ 0.44069209 -0.18594904]]\n"
          ]
        }
      ],
      "source": [
        "Uk, Sk, Vk_t = svd(documents, word_to_ix, 2)\n",
        "print(Uk)\n",
        "print(Sk)\n",
        "print(Vk_t)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCfyqlZEAMLr"
      },
      "source": [
        "##### Expected Output\n",
        "Note: the order of rows could be different in your implementation as it is based on the indexing of the tokens done in build_word_to_ix\n",
        "```CPP \n",
        "[[-0.12717294  0.20278638]\n",
        " [-0.30025142  0.45516076]\n",
        " [-0.11470385 -0.06974269]\n",
        " [-0.40776071  0.39028813]\n",
        " [-0.11406442 -0.09695837]\n",
        " [-0.11406442 -0.09695837]\n",
        " [-0.1610913   0.18784146]\n",
        " [-0.41909101  0.14444841]\n",
        " [-0.26503675  0.13251736]\n",
        " [-0.20868502  0.02493571]\n",
        " [-0.34760786 -0.47741341]\n",
        " [-0.20656126 -0.17503165]\n",
        " [-0.20656126 -0.17503165]\n",
        " [-0.3086385  -0.33652293]\n",
        " [-0.20656126 -0.17503165]\n",
        " [-0.16957527 -0.26134991]]\n",
        "[[4.63682644 0.        ]\n",
        " [0.         3.4660758 ]]\n",
        "[[-0.20485755  0.28230559]\n",
        " [-0.2018926   0.18797401]\n",
        " [-0.27781813  0.33979025]\n",
        " [-0.3848213   0.42056737]\n",
        " [-0.32282413  0.34698447]\n",
        " [-0.49850192 -0.22485343]\n",
        " [-0.3270043  -0.52403905]\n",
        " [-0.14631016 -0.03570198]\n",
        " [-0.4592868  -0.38181953]]\n",
        " ```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLJju9ExAMLr"
      },
      "source": [
        "## <CENTER>Task-2 (10 Points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rouQxivbAMLr"
      },
      "source": [
        "### Visualize documents in 2D space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "rNlxasp3AMLr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "be28f503-1dbf-4787-bc34-8ea2d2f51b73"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAENCAYAAAAMmd6uAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAAsTAAALEwEAmpwYAAAY70lEQVR4nO3df5BdZX3H8fcnwdCmIuFHDGl+La2xnWgZtFfQDghtlja0Y0KnqOCqoUV3qGVKa3+YaTrUYtMRrT9mKm3ZUcfQrg2YDsNWYylZDbSO0NwotQYMiZT8IsCCQH9sIUa+/eOcNXdvzv7KOXvvufd8XjM7557nPHueZ4/4uU+ee85zFRGYmVn3m9PuDpiZWWs48M3MKsKBb2ZWEQ58M7OKcOCbmVWEA9/MrCIc+GZmFeHAt1kjKZp+XpQ0Iukbkj4t6XJJc9vdz26VXvMd7e6Hlccp7e6AVcKfptu5wALgNcC7gGuBuqS+iHikTX0zqwz5SVubLZICICKUcWwR8JfAW4GDQC0inmptD7tbev3vjYhL290XKwdP6VhbRMSTwFXADmAZ8EfNdSStlHSbpMOSjkp6PN1fmXVOSXMlXSfpa5Kel/R/kval00crG+p9Lp3u6Mk4x6XpsQ82le9Iy18m6UZJ35X0gqQ9kt7bUO86Sf+Rtn1I0p9Kyvz/maQLJW2V9ET69x2UdKukH8+oO9b+KZL+SNLedIrsoKSbJc1rqHvN2JstcEnTtNoHG+qtlTQs6Uh6rscl3SvpfVn9tc7nKR1rm4h4SdKfAZcCV0v63Uj/ySnpDcB24DRgCHgI+GngncA6Sb0RsXPsXGngfRG4jORfDJ8H/gvoAX4V+FdgbwHd3gJcCGwDvg9cCQxI+j5wHrA+7ccwsBa4ERgFbm48iaTfAAaAF9O/7yCwEngP8BZJb4yIAxntfx64GPhy+vf9MvCHwCuBX0/rPEgyjfYnwH7gcw2/vyNtvx+4FXgC+Efg6fQc56Xn+asZXRXrDBHhH//Myg8QyX9ik9Y5lSQ4Azg3LRPwcFrW11T/7Wn5d4A5DeV/npYPAadmtLGwYf9zad2ejP5cmh77YFP5jrR8J7CgofwngKPAs8B/Aksaji0gCdIR4JSG8lenv7OvsX56bDXwA+DOCdrfBZzZUP5j6Xl+AJyTcf13THDdd5G82bwy49jZ7f5vxz+z8+MpHWuriHgReCbdXZhuf45kNP/1iBhsqn87yWj9p4CLIJnKAd4H/B9wXXrOcW1ExEhBXd4QEc81nPvRtD8LgA9FxOGGY8+RjJ7PBpY0nOM3gZcBNzTWT39nmORN6y2STsto/wMR8b2G+v8LDJJMz9Zm+LccI3mzHScinp7heaxDeErHymDsQ92xeefXp9uvTFD/KyRh/zrgPpI3h9OBByLi8dnqZKqeUTbW5q6MY2OBvpRkegXgTen2knTqqtkrSe5oenXGObPaP5huz8jq8AQGgY8BD0naAtwLfK3AN0YrIQe+tZWkHwHOTHfHwub0dHtkgl8bK1/QtD18Qs2CRcTzGcXH0u1kx17WUHZWuv2DKZp7eUb7z03SxrSfaYiIj0t6muRfRr8N/A4Qku4F/iAist5YrMN5Ssfa7SKSgceTEfFYWjYWnOdM8DuLm+o9l26XnFg100vpNmvAs2Ca58hjrN+nR4Qm+bl3NjsREbdFxBtJ3oB+BfgM8GbgbkkLJ/1l60gOfGub9HbFjenu5xsOfTPdXjrBr/58uv1Guv0OSeifl3VLY4Zn0+2yjGMznQc/Gfen24tnuZ2XmMaoPyKei4htEfFekg+0zyQJfusyDnxrC0mvJLnF8VLgAMldNmO+BuwBLpJ0ZdPvXUkSlI+QfFhKRPyA5DbCHwX+RtKpTb8zr2nE+m/p9r1N9X4GuCHXHzY9nyL5sPQTkl7dfDDtbxFvBs+Q/aaGpJ+XdMIDcSSfH0ByK6l1Gc/h26xreNhnDseXVrgImEcSvn2Nd4ZEREhaD9wD3C7pLpJR/E8BVwD/Dbw7IsamZiC57/xC4C3AI5K+mNZbBvwiyXz559K6d5Hck3+1pKXAA8ByYF167G2F/fEZIuI76X34nwV2S/onkjewl6X9uJjk84yfztnUMHCVpH8k+dfQ94H7IuI+4E7gfyTdDzxG8sH5xcAbSD4o3p6zbSshB761wp+k26MkIbwfuA34B+Cfm4IbgIh4IL2D5Y+BXpIgfxr4e5LbH/c01T8qaQ1wHfBukgegRHIHzZ2k/xpI674gaTXwFyQPar0B+DbwDuB7zHLgp334O0n/DvweyRTVLwL/m/Z3K3B7Ac3cQHLn02qSB7TmkLwx3gdsAH6J5I6oXwZeIPnf5QPAX0fECbdrWufzWjpmZhXhOXwzs4pw4JuZVYQD38ysIhz4ZmYVUdq7dM4+++zo6elpdzfMzDrKrl27no6IzCelSxv4PT091OtezsPMbCYk7Z/omKd0zMwqwoFvZlYRDnwzs4pw4JuZVYQD38ysIhz4ZmZ5DQ5CTw/MmZNsBwen+o22KO1tmWZmHWFwEPr7YTT9CoH9+5N9gL6+9vUrg0f4ZmZ5bNx4POzHjI4m5SXjwDczy+PAgZmVt5ED38wsj+XLZ1beRg58M7M8Nm2C+fPHl82fn5SXjAPfzCyPvj4YGIAVK0BKtgMDpfvAFnyXjplZfn19pQz4Zh7hm5lVhAPfzKwiHPhmZhXhwDfrRB3yKL+Viz+0Nes0HfQov5WLR/hmnaaDHuW3cikk8CWtkbRH0j5JGyap92uSQlKtiHbNKqmDHuW3cskd+JLmArcAlwOrgKslrcqodxpwA/BA3jbNKq2DHuW3cilihH8BsC8iHo2Io8AWYF1GvQ8BNwMvFNCmWXV10KP8Vi5FBP4S4GDD/qG07IckvR5YFhFfmuxEkvol1SXVR0ZGCuiaWRfqoEf5rVxm/S4dSXOAjwPXTFU3IgaAAYBarRaz2zOzDtYhj/JbuRQxwj8MLGvYX5qWjTkNeC2wQ9JjwBuBIX9wa2bWWkUE/k5gpaRzJc0DrgKGxg5GxPMRcXZE9ERED3A/sDYi6gW0bWZm05Q78CPiGHA9cDfwMHBHROyWdJOktXnPb2ZmxShkDj8itgHbmspunKDupUW0aWZmM+Mnbc3MKsKBb2ZWEQ58M7OKcOCXnZfBNbOCeHnkMvMyuGZWII/wy8zL4JpZgRz4ZeZlcM2sQA78MvMyuGZWIAd+mXkZXDMrkAO/zLwMrpkVyHfplJ2XwTWzgniEb2ZWEQ58M7OKcOCbmVWEA9/MrCIc+GZmFVFI4EtaI2mPpH2SNmQcv07Sf0h6UNK/SlpVRLtmZjZ9uQNf0lzgFuByYBVwdUagfz4ifiYizgc+Anw8b7tmZjYzRYzwLwD2RcSjEXEU2AKsa6wQEf/VsPtjQBTQrpmZzUARD14tAQ427B8CLmyuJOm3gPcD84BfyDqRpH6gH2C514sxMytUyz60jYhbIuIngQ8AfzxBnYGIqEVEbeHCha3qmplZJRQR+IeBZQ37S9OyiWwBriigXTMzm4EiAn8nsFLSuZLmAVcBQ40VJK1s2P0VYG8B7ZqZ2QzknsOPiGOSrgfuBuYCn42I3ZJuAuoRMQRcL6kX+D7wLLA+b7tmZjYzhayWGRHbgG1NZTc2vL6hiHbMzOzk+UlbM7OKcOCbmVWEA9+sWwwOQk8PzJmTbAcH290jKxl/45VZNxgchP5+GB1N9vfvT/bB35hmP+QRvlk32LjxeNiPGR1Nys1SDnyzbnDgwMzKrZIc+GbdYKK1p7wmlTVw4Jt1g02bYP788WXz5yflZikHvlk36OuDgQFYsQKkZDsw4A9sbRzfpWPWLfr6HPA2KY/wzcwqwoFvZlYRDnwzs4pw4JuZVYQD38ysIhz4ZmYVUUjgS1ojaY+kfZI2ZBx/v6SHJH1L0rCkFUW0a2Zm05c78CXNBW4BLgdWAVdLWtVU7ZtALSLOA7YCH8nbrpmZzUwRI/wLgH0R8WhEHAW2AOsaK0TEVyNibCm/+4GlBbRrZmYzUETgLwEONuwfSssmci3w5awDkvol1SXVR0ZGCuiamZmNaemHtpLeCdSAj2Ydj4iBiKhFRG3hwoWt7JqZWdcrYi2dw8Cyhv2ladk4knqBjcAlEfFiAe2amdkMFDHC3wmslHSupHnAVcBQYwVJrwNuBdZGxFMFtGlmZjOUO/Aj4hhwPXA38DBwR0TslnSTpLVptY8CLwe+IOlBSUMTnM7MzGZJIcsjR8Q2YFtT2Y0Nr3uLaMfMzE6en7Q1M6sIB76ZWUU48M3MKsKBb2ZWEQ58M7OKcOCbmVWEA9/MrCIc+GZmFeHANzOrCAe+mVlFOPCtMwwOQk8PzJmTbAcH290js45TyFo6ZrNqcBD6+2E0/dK0/fuTfYC+vvb1y6zDeIRv5bdx4/GwHzM6mpSb2bQ58K38DhyYWbmZZXLgW/ktXz6zcjPL5MC38tu0CebPH182f35SbmbTVkjgS1ojaY+kfZI2ZBx/s6RvSDom6coi2rQK6euDgQFYsQKkZDsw4A9szWYo9106kuYCtwCXAYeAnZKGIuKhhmoHgGuA38/bnlVUX58D3iynIm7LvADYFxGPAkjaAqwDfhj4EfFYeuylAtozM7OTUMSUzhLgYMP+obRsxiT1S6pLqo+MjBTQNcuttzeZRhn76fXXE5t1qlJ9aBsRAxFRi4jawoUL290d6+2F4eHxZcPDDn2zDlVE4B8GljXsL03LrNM1h/1U5WXj5RjMxiliDn8nsFLSuSRBfxXwjgLOa3byvByD2Qlyj/Aj4hhwPXA38DBwR0TslnSTpLUAkt4g6RDwVuBWSbvztms2KS/HYHaCQhZPi4htwLamshsbXu8kmeqxTrJ6dfb0zerVre/LTHk5BrMTlOpDWyuZ7dtPDPfVq5PysvNyDGYncODb5LZvh4jjP50Q9uDlGKy1OuQGAQe+dScvx2CtMnaDwP79yaBo7AaBEoa+IqLdfchUq9WiXq+3uxtmZpPr6UlCvtmKFfDYY63uDZJ2RUQt65hH+GZmeXTQDQIOfDOzPDroBgEHvplZHh10g4AD38wsjw66QaCQB6/MzCqtQ76vwSN8M7OKcOCbmVWEA9/MrCIc+GZmFeHAN+tEHbJ2i5WL79Ix6zT+chc7SR7hm3Uaf7mLnaRCAl/SGkl7JO2TtCHj+KmSbk+PPyCpp4h2zSqpg9ZusXLJHfiS5gK3AJcDq4CrJa1qqnYt8GxEvAr4BHBz3nbNKquD1m6xcilihH8BsC8iHo2Io8AWYF1TnXXA5vT1VmC1JBXQtln1dNDaLVYuRQT+EuBgw/6htCyzTvql588DZzWfSFK/pLqk+sjISAFdM+tCHbR2i5VLqe7SiYgBYACSL0Bpc3fMyqtD1m6xcilihH8YWNawvzQty6wj6RTgdOCZAto2M7NpKiLwdwIrJZ0raR5wFTDUVGcIWJ++vhL4SpT1uxXNzLpU7imdiDgm6XrgbmAu8NmI2C3pJqAeEUPAZ4C/lbQP+B7Jm4KZmbVQIXP4EbEN2NZUdmPD6xeAtxbRlpmZnRw/aWtmVhEOfDOzinDgm5lVhAPfzKwiHPhmZhXhwDczqwgHvplZRTjwzcwqwoFvZlYRDnwzs4pw4JuZVYQD38ysIhz4ZmYV4cA3M6sIB76ZWUU48M3MKiJX4Es6U9I9kvam2zMmqPdPkp6T9MU87ZmZ2cnLO8LfAAxHxEpgON3P8lHgXTnbMjOzHPIG/jpgc/p6M3BFVqWIGAb+O2dbZmaWQ97AXxQRR9LXTwCL8pxMUr+kuqT6yMhIzq6ZmVmjKb/EXNJ24JyMQxsbdyIiJEWezkTEADAAUKvVcp3LzMzGmzLwI6J3omOSnpS0OCKOSFoMPFVo78zMrDB5p3SGgPXp6/XAXTnPZ2ZmsyRv4H8YuEzSXqA33UdSTdKnxypJ+hfgC8BqSYck/VLOds3MbIamnNKZTEQ8A6zOKK8D72nYvzhPO2Zmlp+ftDUzqwgHvplZRTjwzcwqwoFvZlYRDnwzs4pw4JuZVYQD38ysIhz4ZmYV4cA3M6sIB76ZWUU48M3MKsKBb2ZWEQ58M7OKcOCbmZXF4CD09MCcOcl2cLDQ0+daHtnMzAoyOAj9/TA6muzv35/sA/T1FdKER/hmZmWwcePxsB8zOpqUFyRX4Es6U9I9kvam2zMy6pwv6euSdkv6lqS352nTzKwrHTgws/KTkHeEvwEYjoiVwHC632wUeHdEvAZYA3xS0oKc7ZqZdZfly2dWfhLyBv46YHP6ejNwRXOFiHgkIvamrx8HngIW5mzXzKy7bNoE8+ePL5s/PykvSN7AXxQRR9LXTwCLJqss6QJgHvDdCY73S6pLqo+MjOTsmplZB+nrg4EBWLECpGQ7MFDYB7YAiojJK0jbgXMyDm0ENkfEgoa6z0bECfP46bHFwA5gfUTcP1XHarVa1Ov1qaqZmVkDSbsiopZ1bMrbMiOid5ITPylpcUQcSQP9qQnqvQL4ErBxOmFvZmbFyzulMwSsT1+vB+5qriBpHnAncFtEbM3ZnpmZnaS8gf9h4DJJe4HedB9JNUmfTuu8DXgzcI2kB9Of83O2a2ZmMzTlHH67eA7fzGzmJpvD95O2ZmYV4cA3M6sIB76ZWUU48M3MKsKBb2blNstrxFeJ18M3s/JqwRrxVeIRvpmVVwvWiK8SB76ZlVcL1oivEge+mZVXC9aIrxIHvpmVVwvWiK8SB76ZlVcL1oivEt+lY2bl1tfngC+IR/hmZhXhwDczqwgHvlk381Oq1sBz+Gbdyk+pWpNcI3xJZ0q6R9LedHvCF5hLWiHpG+k3Xe2WdF2eNqfkEY1Zwk+pWpO8UzobgOGIWAkMp/vNjgBviojzgQuBDZJ+PGe72cZGNPv3Q8TxEY1D36rIT6lak7yBvw7YnL7eDFzRXCEijkbEi+nuqQW0OTGPaMyO81Oq1iRv+C6KiCPp6yeARVmVJC2T9C3gIHBzRDw+Qb1+SXVJ9ZGRkZn3xiMas+P8lKo1mTLwJW2X9O2Mn3WN9SL5NvTMb0SPiIMRcR7wKmC9pMw3hogYiIhaRNQWLlw487/GIxqz4/yUqjWZ8i6diOid6JikJyUtjogjkhYDT01xrsclfRu4GNg6495OZdOm8XclgEc0Vm1+StUa5J3SGQLWp6/XA3c1V5C0VNKPpq/PAC4C9uRsN5tHNGZmE1IyE3OSvyydBdwBLAf2A2+LiO9JqgHXRcR7JF0GfIxkukfApyJiYKpz12q1qNfrJ903M7MqkrQrImpZx3I9eBURzwCrM8rrwHvS1/cA5+Vpx8zM8vPSCmZ2Ij/A2JW8tIKZjeclGbqWR/hmNp4fYOxaDnwzG88PMHYtB76ZjecHGLuWA9/MxvOSDF3LgW9m4/kBxq7lu3TM7ERekqEreYRvZlYRDnwzs4pw4JuZVYQD38ysIhz4ZmYVkWt55NkkaYRkyeW8zgaeLuA83crXZ3K+PpPz9ZlYu67NiojI/MrA0gZ+USTVJ1ob2nx9puLrMzlfn4mV8dp4SsfMrCIc+GZmFVGFwJ/y6xQrztdncr4+k/P1mVjprk3Xz+GbmVmiCiN8MzPDgW9mVhldF/iSzpR0j6S96faMSeq+QtIhSZ9qZR/baTrXR9L5kr4uabekb0l6ezv62kqS1kjaI2mfpA0Zx0+VdHt6/AFJPW3oZltM49q8X9JD6X8rw5JWtKOf7TLV9Wmo92uSQlLbbtXsusAHNgDDEbESGE73J/Ih4L6W9Ko8pnN9RoF3R8RrgDXAJyUtaF0XW0vSXOAW4HJgFXC1pFVN1a4Fno2IVwGfAG5ubS/bY5rX5ptALSLOA7YCH2ltL9tnmtcHSacBNwAPtLaH43Vj4K8DNqevNwNXZFWS9LPAIuCfW9Ot0pjy+kTEIxGxN339OPAUkPnkXpe4ANgXEY9GxFFgC8l1atR43bYCqyWphX1slymvTUR8NSLGvvX8fmBpi/vYTtP5bweSweXNwAut7Fyzbgz8RRFxJH39BEmojyNpDvAx4Pdb2bGSmPL6NJJ0ATAP+O5sd6yNlgAHG/YPpWWZdSLiGPA8cFZLetde07k2ja4FvjyrPSqXKa+PpNcDyyLiS63sWJaO/MYrSduBczIObWzciYiQlHXf6fuAbRFxqBsHaQVcn7HzLAb+FlgfES8V20vrNpLeCdSAS9rdl7JIB5cfB65pc1eADg38iOid6JikJyUtjogjaWA9lVHtTcDFkt4HvByYJ+l/ImKy+f6OUcD1QdIrgC8BGyPi/lnqalkcBpY17C9Ny7LqHJJ0CnA68ExrutdW07k2SOolGVBcEhEvtqhvZTDV9TkNeC2wIx1cngMMSVobEfWW9TLVjVM6Q8D69PV64K7mChHRFxHLI6KHZFrntm4J+2mY8vpImgfcSXJdtrawb+2yE1gp6dz0b7+K5Do1arxuVwJfiWo8tTjltZH0OuBWYG1EZA4gutik1ycino+IsyOiJ82b+0muU8vDHroz8D8MXCZpL9Cb7iOpJunTbe1ZOUzn+rwNeDNwjaQH05/z29LbFkjn5K8H7gYeBu6IiN2SbpK0Nq32GeAsSfuA9zP53V9dY5rX5qMk/1L+QvrfSvObZdea5vUpDS+tYGZWEd04wjczswwOfDOzinDgm5lVhAPfzKwiHPhmZhXhwDczqwgHvplZRfw/RoOJh7QZx4gAAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "#Visualize documents and print coordinates\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.linalg as la\n",
        "\n",
        "\n",
        "plt.scatter(Vk_t[:,0], Vk_t[:,1], c='red')\n",
        "plt.title('Documents', size=20)\n",
        "plt.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8qOoeIfJzDt"
      },
      "source": [
        "##### Expected\n",
        "```CPP\n",
        "[[-0.94988891  0.97849257]\n",
        " [-0.93614095  0.65153215]\n",
        " [-1.28819443  1.17773875]\n",
        " [-1.78434956  1.4577184 ]\n",
        " [-1.49687945  1.20267446]\n",
        " [-2.31146688 -0.77935904]\n",
        " [-1.51626218 -1.81635909]\n",
        " [-0.67841483 -0.12374577]\n",
        " [-2.12963317 -1.32341543]]\n",
        "```\n",
        "![image.png](attachment:9df44069-ca33-481c-856c-0ca163d59c10.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8tJC6EdAMLr"
      },
      "source": [
        "### Visualize terms in 2D space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "vF1c71HSAMLr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "9a9ce697-f7a5-4f12-b0e4-44b2ed7999f1"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "#Visualize terms and print coordinates\n",
        "plt.scatter(Uk[:,0], Uk[:,1], c='red')\n",
        "plt.title('Terms', size=20)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygvNW2d_JzDw"
      },
      "source": [
        "##### Expected\n",
        "Note: the order of rows could be different in your implementation as it is based on the indexing of the tokens done in build_word_to_ix\n",
        "```CPP\n",
        "[[-0.58967885  0.70287296]\n",
        " [-1.3922137   1.57762168]\n",
        " [-0.53186185 -0.24173347]\n",
        " [-1.89071562  1.35276825]\n",
        " [-0.5288969  -0.33606505]\n",
        " [-0.5288969  -0.33606505]\n",
        " [-0.74695242  0.65107273]\n",
        " [-1.94325227  0.50066913]\n",
        " [-1.22892939  0.45931522]\n",
        " [-0.96763621  0.08642905]\n",
        " [-1.61179732 -1.65475107]\n",
        " [-0.95778872 -0.60667296]\n",
        " [-0.95778872 -0.60667296]\n",
        " [-1.43110318 -1.16641399]\n",
        " [-0.95778872 -0.60667296]\n",
        " [-0.7862911  -0.90585858]]\n",
        "```\n",
        "![term.png](attachment:bdb847ce-c276-45a9-a7fe-a89787bbedfa.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLfzNNlKAMLr"
      },
      "source": [
        "## <CENTER>Task-3 (10 Points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCGdw5FKAMLr"
      },
      "source": [
        "### Find matching documents for given document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "3WGgJK4TAMLr"
      },
      "outputs": [],
      "source": [
        "# Hint create query vector for input document. Calculate its cosine distance from other documents\n",
        "import scipy.linalg as la\n",
        "import math\n",
        "\n",
        "\n",
        "def query(s, Uk, Sk, Vk_t, word_to_ix, documents, min_score=0.9):\n",
        "    '''\n",
        "    Input:\n",
        "        s:query document.\n",
        "        Uk:Term matrix\n",
        "        Sk:singular value matrix\n",
        "        Vk_t:Document matrix\n",
        "        word_to_ix: {word, index} map\n",
        "        documents:list of document\n",
        "        min_score:min score beyond which documents are considerd matching\n",
        "    Output:\n",
        "        q_hat: coordinates of query vector\n",
        "        matches: list of tuples containing matching document and its score\n",
        "    '''\n",
        "    #not sure where to get query vec, probably from Vk_t\n",
        "    # need to return a doc from documents with the cos_dist in matches but how\n",
        "    matches = []\n",
        "    \n",
        "    A = Uk@la.diagsvd(Sk, 2, 16)@Vk_t\n",
        "    \n",
        "     \n",
        "\n",
        "    tokens = s.split()\n",
        "    words = set(tokens)\n",
        "    wordVec = [0] * len(A[:,0])\n",
        "\n",
        "    for doc in documents:\n",
        "        tokensFromDocs = doc.split()\n",
        "        tokensFromS = s.split()\n",
        "        words = set(tokensFromDocs + tokensFromS)\n",
        "        vec1 = [0] * len(words)\n",
        "        vec2 = [0] * len(words)\n",
        "\n",
        "        for i, word in enumerate(words):\n",
        "            vec1[i] = tokensFromDocs.count(word)\n",
        "            vec2[i] = tokensFromS.count(word)\n",
        "\n",
        "        dot_product = sum(vec1[i] * vec2[i] for i in range(len(vec1)))\n",
        "        mag1 = math.sqrt(sum(vec1[i] ** 2 for i in range(len(vec1))))\n",
        "        mag2 = math.sqrt(sum(vec2[i] ** 2 for i in range(len(vec2))))\n",
        "        \n",
        "\n",
        "        cosine_distance = 1 - (dot_product / (mag1 * mag2))\n",
        "\n",
        "        if(cosine_distance >= min_score):\n",
        "            matches.append((doc, cosine_distance))\n",
        "\n",
        "        q_hat = Vk_t[documents.index(doc)]\n",
        "\n",
        "          \n",
        "\n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "    return q_hat, matches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "SRqCh5PsAMLs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e737796c-4cd8-4585-ca8a-a3e34694e469"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 0.27558757 -0.25154893]\n",
            "[('One of the most prominent applications of computer vision is in autonomous vehicles, where it helps the vehicle \"see\" and make decisions based on its surroundings.', 0.9449518117436819), ('Cybersecurity is also essential in protecting critical infrastructure, such as power grids and transportation systems, from cyber attacks that could cause significant disruptions.', 0.9371305386538068)]\n"
          ]
        }
      ],
      "source": [
        "q_hat, matches = query('E-commerce companies use cybersecurity to protect online transactions and prevent fraud.', Uk, Sk, Vk_t, word_to_ix, documents, 0.9)\n",
        "print(q_hat)\n",
        "if matches is not None:\n",
        "    print(matches)\n",
        "else:\n",
        "    print(\"No matches found\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f44WDNKPAMLs"
      },
      "source": [
        "##### Expected\n",
        "```CPP\n",
        "[-0.51519977 -0.51155458]\n",
        "[('One of the most important applications of cybersecurity is in safeguarding sensitive data and personal information, such as financial data or healthcare records.', 0.9956454663736864), ('In the healthcare industry, cybersecurity is used to protect medical devices and prevent unauthorized access to patient data.', 0.9746106473873951)]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_n9ayNLAMLs"
      },
      "source": [
        "### Visual representation of query/document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oFlw3alvAMLs"
      },
      "outputs": [],
      "source": [
        "# Plot terms, documents and query documents along with lines representing its cosine angle "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMIYt7IWJzD0"
      },
      "source": [
        "##### Expected\n",
        "![Cosine angle.png](attachment:6f3fc59f-73c5-48ff-930f-9d53a78a5dd5.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsDfmhqUn5MS"
      },
      "source": [
        "## **Theory questions: (5 points)**\n",
        "- A) Give short description of Left-eigen vectors, right-eigen vectors and eigen-values matrix retured by Singular Value Decompostion of document-terms count matrix.\n",
        "\n",
        "**Answer**: The singular value decomposition of a matrix is the factorization of A into the product of three matrices U, S, and V where the columns of U and V are orthonormal and the matrix D is diagonal with positive real entries.\n",
        "\n",
        "- B) Visually represent the document \"Graph and tree generation\" in 2D space along with words and documents as given in previous question."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mM07ZgklplWG"
      },
      "source": [
        "# Q2. n-Gram Language Models (35 points)\n",
        "\n",
        "Your task is to train n-gram language models. [Ref SLP Chapter 3]\n",
        "\n",
        "- Task 1: You will train unigram, bigram, and trigram models on given training files. Then you will score on given test files for unigram, bigram, and trigram. you will generate sentences from the trained model and compute perplexity.\n",
        "- Task 2: You will create training data for n > 3. and Repeat the above task from training model.\n",
        "<h6>Part-A = (55 Points) </h6>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "qpO0AKC8plWH"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Your imports go here\n",
        "You are encouraged to implement your own functions and not use from library.\n",
        "'''\n",
        "import sys\n",
        "from collections import Counter\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "iS3J6hisvReS"
      },
      "outputs": [],
      "source": [
        "# constants to define pseudo-word tokens\n",
        "# access via UNK, for instance\n",
        "# for this assignemnt we will follow <s> tag for beginning of sentence and\n",
        "# </s> for end of sentence as suggested in SLP Book. Check sample training files for reference.\n",
        "UNK = \"<UNK>\"\n",
        "SENT_BEGIN = \"<s>\"\n",
        "SENT_END = \"</s>\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zp00BMTJ4f8J"
      },
      "source": [
        "We need to initialise global variables for model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "W_VUIkeUplWI"
      },
      "outputs": [],
      "source": [
        "\n",
        "\"\"\"Initializes Parameters:\n",
        "  n_gram (int): the n-gram order.\n",
        "  is_laplace_smoothing (bool): whether or not to use Laplace smoothing\n",
        "  threshold: words with frequency below threshold will be converted to token\n",
        "\"\"\"\n",
        "# Initializing different object attributes\n",
        "n_gram = 2\n",
        "is_laplace_smoothing = True\n",
        "vocab = [] \n",
        "n_gram_counts = {}\n",
        "n_minus_1_gram_counts = None\n",
        "threshold = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRYDPhLAplWI"
      },
      "source": [
        "\n",
        "### Implement training function (10 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "X4L2wHUjvciI"
      },
      "outputs": [],
      "source": [
        "def make_ngrams(tokens: list, n: int) -> list:\n",
        "    \"\"\"Creates n-grams for the given token sequence.\n",
        "    Args:\n",
        "    tokens (list): a list of tokens as strings\n",
        "    n (int): the length of n-grams to create\n",
        "\n",
        "    Returns:\n",
        "    list: list of tuples of strings, each tuple being one of the individual n-grams\n",
        "    \"\"\"\n",
        "    n_grams = []\n",
        "\n",
        "    # Loop through words, creating n-grams\n",
        "    for i in range((len(tokens)) - n + 1):\n",
        "        ngram = \" \".join(tokens[i:i+n])\n",
        "        n_grams.append(ngram)\n",
        "\n",
        "    return n_grams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "LHOwrc2Uvgtm"
      },
      "outputs": [],
      "source": [
        "def train(training_file_path):\n",
        "    \"\"\"Trains the language model on the given data. Input file that\n",
        "    has tokens that are white-space separated, has one sentence per line, and\n",
        "    that the sentences begin with <s> and end with </s>\n",
        "    Parameters:\n",
        "      training_file_path (str): the location of the training data to read\n",
        "\n",
        "    Returns:\n",
        "    N Gram Counts, Vocab, N Minus 1 Gram Counts\n",
        "    \"\"\"\n",
        "    with open(training_file_path, 'r') as fh:\n",
        "      content = fh.read().split() # Read and split data to get list of words\n",
        "    \n",
        "    # Get the count of each word\n",
        "    vocab = set(content)\n",
        "    n_minus_1_gram_counts = []\n",
        "    \n",
        "    # Replace the words with <UNK> if count is < threshold(=1)\n",
        "    for word in vocab:\n",
        "       if(content.count(word) < 2):\n",
        "          word = UNK\n",
        "    # make use of make_n_grams function\n",
        "    n_grams = make_ngrams(content, n_gram)\n",
        "    # Get the training data vocabulary\n",
        "    # For n>1 grams compute n-1 gram counts to compute probability\n",
        "\n",
        "    for ngram in n_grams:\n",
        "      n_gram_counts[ngram] = content.count(ngram) + 1\n",
        "      n_minus_1_gram_counts.append((content.count(ngram) + 1)/len(n_grams))\n",
        "\n",
        "    return n_gram_counts, vocab, n_minus_1_gram_counts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ucWPcHIJly_"
      },
      "source": [
        "Output your Trained Data Parameters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "P7j8ct0BwocM",
        "jupyter": {
          "outputs_hidden": true
        },
        "outputId": "5f88ef6f-5909-454e-918d-3a4b908a807f",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'The dragonfly knows': 1, 'dragonfly knows where': 1, 'knows where it': 1, 'where it is': 1, 'it is at': 1, 'is at all': 1, 'at all times': 1, 'all times by': 1, 'times by subtracting': 1, 'by subtracting where': 1, 'subtracting where it': 1, 'where it was': 1, 'it was from': 1, 'was from where': 1, 'from where it': 1, 'it is not': 1, 'is not The': 1, 'not The amount': 1, 'The amount of': 1, 'amount of territorial': 1, 'of territorial disputes': 1, 'territorial disputes in': 1, 'disputes in the': 1, 'in the South': 1, 'the South China': 1, 'South China Sea': 1, 'China Sea will': 1, 'Sea will be': 1, 'will be devastating': 1, 'be devastating for': 1, 'devastating for the': 1, 'for the economy': 1, 'the economy by': 1, 'economy by 2030': 1, 'by 2030 A': 1, '2030 A single': 1, 'A single atom': 1, 'single atom is': 1, 'atom is mostly': 1, 'is mostly empty': 1, 'mostly empty space,': 1, 'empty space, but': 1, 'space, but what': 1, 'but what is': 1, 'what is between': 1, 'is between that': 1, 'between that space?': 1, 'that space? The': 1, 'space? The dog': 1, 'The dog from': 1, 'dog from Blue’s': 1, 'from Blue’s clues': 1, 'Blue’s clues is': 1, 'clues is a': 1, 'is a hoot': 1, 'a hoot The': 1, 'hoot The show': 1, 'The show is': 1, 'show is from': 1, 'is from the': 1, 'from the 1990s,': 1, 'the 1990s, a': 1, '1990s, a really': 1, 'a really old': 1, 'really old decade': 1, 'old decade Tool': 1, 'decade Tool can': 1, 'Tool can refer': 1, 'can refer to': 1, 'refer to the': 1, 'to the band': 1, 'the band or': 1, 'band or a': 1, 'or a tool,': 1, 'a tool, a': 1, 'tool, a tool': 1, 'a tool can': 1, 'tool can be': 1, 'can be defined': 1, 'be defined many': 1, 'defined many different': 1, 'many different ways': 1, 'different ways Oscar': 1, 'ways Oscar Pistorius': 1, 'Oscar Pistorius is': 1, 'Pistorius is a': 1, 'is a hoot,': 1, 'a hoot, I': 1, 'hoot, I love': 1, 'I love everything': 1, 'love everything he': 1, 'everything he is': 1, 'he is in': 1}\n",
            "{'of', 'really', 'dragonfly', 'Sea', 'mostly', 'that', 'decade', 'at', 'Pistorius', 'tool', 'but', 'dog', 'show', 'can', 'ways', 'Oscar', 'everything', 'clues', 'it', 'space?', 'many', 'South', 'subtracting', 'A', 'the', 'China', 'empty', 'be', 'devastating', 'he', '1990s,', 'old', 'to', 'economy', 'in', 'or', 'hoot,', 'times', 'Blue’s', 'for', 'The', 'between', 'disputes', 'where', 'what', 'will', 'Tool', 'all', 'single', 'love', 'tool,', 'amount', 'is', 'a', 'by', '2030', 'space,', 'refer', 'defined', 'hoot', 'was', 'I', 'knows', 'band', 'not', 'different', 'atom', 'territorial', 'from'}\n"
          ]
        }
      ],
      "source": [
        "n_gram_counts, vocab, n_minus_1_gram_counts = train(\"/content/train.txt\")\n",
        "print(n_gram_counts)\n",
        "print(vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54hLufsd5lVZ"
      },
      "source": [
        "### Scoring function (points 5):\n",
        "Implement Score function that will take input sentence and output probability of given string representing a single sentence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "qqJ05JFSvxIr"
      },
      "outputs": [],
      "source": [
        "def score(sentence):\n",
        "    \"\"\"Calculates the probability score for a given string representing a single sentence.\n",
        "    Parameters:\n",
        "      sentence (str): a sentence with tokens separated by whitespace to calculate the score of\n",
        "      \n",
        "    Returns:\n",
        "      float: the probability value of the given string for this model\n",
        "    \"\"\"\n",
        "    # Split the input sentence and replace out of vocabulary tokens with <UNK>  \n",
        "    tokens = sentence.split()  \n",
        "    for word in tokens:\n",
        "        if word not in vocab:\n",
        "            word = UNK\n",
        "    # Calculate probability for each word and multiply(or take log and sum) them to get the sentence probability\n",
        "    probability  = sum(tokens.count(word)/len(tokens) for word in tokens)\n",
        "    return probability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "QaayxCgOzUUb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3a7010a-3462-4850-d18e-4365b1c674d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# of test sentences:  8\n"
          ]
        }
      ],
      "source": [
        "with open(\"train.txt\", 'r') as fh:\n",
        "    test_content = fh.read().split(\"\\n\")\n",
        "num_sentences_1 = len(test_content)\n",
        "ten_sentences_1 = test_content[:10]\n",
        "print(\"# of test sentences: \", num_sentences_1)\n",
        "probablities = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "ei2Zrd61zeKq"
      },
      "outputs": [],
      "source": [
        "# print probabilities/score of sentences in test content\n",
        "for sentence in test_content:\n",
        "  probablities.append(score(sentence))\n",
        "probablities = np.array(probablities)\n",
        "mean = np.mean(probablities)\n",
        "std_dev = np.std(probablities)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awEQeJtIzTrC"
      },
      "source": [
        "### Sentence generation (10 points)\n",
        "Generate sentence from the above trained model\n",
        "- To generate next word from a set of probable n-grams and their probabilities check below tutorial:\n",
        "https://numpy.org/doc/stable/reference/random/generated/numpy.random.choice.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "SpTKpxDMv2-4"
      },
      "outputs": [],
      "source": [
        "def generate_sentence():\n",
        "    \"\"\"Generates a single sentence from a trained language model using the Shannon technique.\n",
        "      \n",
        "    Returns:\n",
        "      str: the generated sentence\n",
        "    \"\"\"\n",
        "    # Start with <s> and randomly generate words until we encounter sentence end\n",
        "    n_gram_counts, vocab, n_minus_1_gram_counts = train(\"train.txt\")\n",
        "\n",
        "    keys = [] \n",
        "    for i in n_gram_counts.keys(): \n",
        "      keys.append(i)\n",
        "    \n",
        "    sentence = []\n",
        "    counts = []\n",
        "    prev_word = np.random.choice(len(vocab))\n",
        "    \n",
        "    n = n_gram\n",
        "\n",
        "    # Append sentence begin markers for n>2\n",
        "    if(n>2):\n",
        "      sentence.append(prev_word) #or SENT_BEGIN\n",
        "\n",
        "    # Keep track of previous word for stop condition\n",
        "    if n > 1:\n",
        "      while prev_word != SENT_END:\n",
        "        # Construct the (n-1) gram so far so get a random n minus 1 gram\n",
        "  \n",
        "        n_minus_1_gram = np.random.choice(keys, 1)\n",
        "        print(n_minus_1_gram)\n",
        "        # Get the counts of all available choices based on n-1 gram\n",
        "        \n",
        "        for choice in n_minus_1_gram:\n",
        "          counts.append(list(n_minus_1_gram).count(choice))\n",
        "        print(counts)\n",
        "        # Convert the counts into probability for random.choice() function\n",
        "        probabilities = [x / len(n_minus_1_gram) for x in counts]\n",
        "        newWord = np.random.choice(n_minus_1_gram, 1, p=probabilities)\n",
        "        sentence.append(prev_word)\n",
        "        # If <s> is generated, ignore and generate another word\n",
        "        if(newWord == SENT_BEGIN):\n",
        "          continue\n",
        "        prev_word = newWord\n",
        "    else:\n",
        "      # In case of unigram model, n-1 gram is just the previous word and possible choice is whole vocabulary\n",
        "      while prev_word != SENT_END:\n",
        "        # Convert the counts into probability for random.choice() function\n",
        "        \n",
        "        for choice in vocab:\n",
        "          counts.append(list(vocab).count(choice))\n",
        "       \n",
        "        probabilities = [x / len(vocab) for x in counts]\n",
        "        if(len(vocab) != len(probabilities)): break\n",
        "        newWord = np.random.choice(len(vocab), 1, p=probabilities)\n",
        "        sentence.append(prev_word)\n",
        "        # If <s> is generated, ignore and generate another word\n",
        "        if(newWord == SENT_BEGIN):\n",
        "          continue\n",
        "        prev_word = newWord\n",
        "\n",
        "    # Append sentence end markers for n>2\n",
        "    sentence.append(SENT_BEGIN)\n",
        "    \n",
        "    return sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "aeQKR_UHv462"
      },
      "outputs": [],
      "source": [
        "def generate(n):\n",
        "    \"\"\"Generates n sentences from a trained language model using the Shannon technique.\n",
        "    Parameters:\n",
        "      n (int): the number of sentences to generate\n",
        "      \n",
        "    Returns:\n",
        "      list: a list containing strings, one per generated sentence\n",
        "    \"\"\"\n",
        "    sentences = []\n",
        "    for i in range(n):\n",
        "    # Generate sentences one by one and store\n",
        "      sentences.append(generate_sentence())\n",
        "      \n",
        "    return sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "2UMoLPcvBKpn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "outputId": "3a305ccb-bdcf-4773-8bdc-828e52a5fd1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['defined many']\n",
            "[1]\n",
            "['A single']\n",
            "[1, 1]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-79-4667589289a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Sentences:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-39-fa2511d75447>\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(n)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Generate sentences one by one and store\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m       \u001b[0msentences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerate_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-78-aef3a0517321>\u001b[0m in \u001b[0;36mgenerate_sentence\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m# Convert the counts into probability for random.choice() function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mprobabilities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_minus_1_gram\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcounts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mnewWord\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_minus_1_gram\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprobabilities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;31m# If <s> is generated, ignore and generate another word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: 'a' and 'p' must have same size"
          ]
        }
      ],
      "source": [
        "sentences = generate(50)\n",
        "print(\"Sentences:\")\n",
        "for sentence in sentences:\n",
        "  print(sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-ok3UZEKmFi"
      },
      "source": [
        "### Evaluate model perplexity (5 points)\n",
        "Measures the perplexity for the test sequence with your trained model. \n",
        "you may assume that this sequence may consist of many sentences \"glued together\"\n",
        "\n",
        "The perplexity of the given sequence is the inverse probability of the test set, normalized by the number of words.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "G8mjYygFv7Gq"
      },
      "outputs": [],
      "source": [
        "# Since this sequence will cross many sentence boundaries, we need to include \n",
        "# the begin- and end-sentence markers <s> and </s> in the probability computation. \n",
        "# We also need to include the end-of-sentence marker </s> \n",
        "# but not the beginning-of-sentence marker <s>) in the total count of word tokens N\n",
        "\n",
        "def perplexity(test_sequence):\n",
        "    \"\"\".\n",
        "    Parameters:\n",
        "      test_sequence (string): a sequence of space-separated tokens to measure the perplexity of\n",
        "\n",
        "    Returns:\n",
        "      float: the perplexity of the given sequence\n",
        "    \"\"\" \n",
        "\n",
        "    # Replace out of vocab words with <UNK>, already done in score function\n",
        "    #test_sequence = [token if token in vocab else UNK for token in test_sequence.split()]\n",
        "\n",
        "    # Get the probability for the sequence\n",
        "    test_sequence_score = score(test_sequence)\n",
        "    # Remove sentence begin markers from data for computing N\n",
        "    count = sum([1 for token in test_sequence if token != \"<s>\"])\n",
        "\n",
        "    perplexity = 1/(pow(test_sequence_score, (1/count)))\n",
        "    \n",
        "\n",
        "    return perplexity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tsldHtvIBlTV",
        "outputId": "981b0b5c-ff77-4777-c98b-12ffe0f9d529"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "78.4934782125397\n"
          ]
        }
      ],
      "source": [
        "print(perplexity(\" \".join(sentences[0:10])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfbHtUCW66Qk"
      },
      "source": [
        "### **Explore and explain: (5 points)**\n",
        "* Experiment n_gram model for n = [1,2,3..7] of your choice. Explain the best choice of n that generates more meaninful sentences.\n",
        "\n",
        "**ANSWER**: It seemed to me that smaller values of n such as 3, 4, or 5 were good enough for the training data. Higher than that and the model tried to over-extract meaning from sentences and sentence generation became more esoteric.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llPpoxB4hnAx"
      },
      "source": [
        "# Q3. Classification using LSTM - using Tensorflow (30 Points)\n",
        "In this part, we will be building a bidirectional LSTM network to train and inference sentiment analysis on IMDB dataset.<br>\n",
        "\n",
        "If you need a refresher or have never worked with Neural Networks before, here are a few resources:\n",
        "- https://web.stanford.edu/~jurafsky/slp3/7.pdf\n",
        "- https://web.stanford.edu/~jurafsky/slp3/9.pdf\n",
        "- https://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
        "\n",
        "\n",
        "Training a neural network model will take time. \n",
        "- You can use Google Colab / Kaggle notebooks. You get a free GPU for a limited time to tweak your hyperparameters.\n",
        "- Without a GPU, You might have to wait longer to experiment."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras.utils\n",
        "!pip install keras.preprocessing"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4U-vJ-OhkJa",
        "outputId": "60092ce0-e707-40d0-f6b6-a4978db4737c"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: keras.utils in /usr/local/lib/python3.9/dist-packages (1.0.13)\n",
            "Requirement already satisfied: Keras>=2.1.5 in /usr/local/lib/python3.9/dist-packages (from keras.utils) (2.11.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting keras.preprocessing\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 KB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.9/dist-packages (from keras.preprocessing) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.9/dist-packages (from keras.preprocessing) (1.22.4)\n",
            "Installing collected packages: keras.preprocessing\n",
            "Successfully installed keras.preprocessing-1.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t40hndZFhp1S"
      },
      "source": [
        "### Library Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "zS0zc2NMc3tU"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "from keras.utils import pad_sequences\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AqE0CRtYZEy"
      },
      "source": [
        "### Visualizing data distribution (1 Point)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BDnlJxRtkMz1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "57055717-17f5-4df0-9f2f-5efe043c5226"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-b9290e39-29d2-4cdb-8efa-6ad5b92d3f9f\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-b9290e39-29d2-4cdb-8efa-6ad5b92d3f9f\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import io\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "\n",
        "dataset = pd.read_csv(io.StringIO(uploaded['movie_reviews-2.csv'].decode('utf-8')), sep = ',', encoding = 'latin-1', usecols = lambda col: col not in [\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "tpXhxuORkUi4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a32ecc9-d0cc-45ce-da83-2d68762b2b65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                              review sentiment\n",
            "0  One of the other reviewers has mentioned that ...  positive\n",
            "1  A wonderful little production. <br /><br />The...  positive\n",
            "2  I thought this was a wonderful way to spend ti...  positive\n",
            "3  Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
            "4  Probably my all-time favorite movie, a story o...  positive\n"
          ]
        }
      ],
      "source": [
        " #######################################################   \n",
        " # print head of data frame with help of head function # \n",
        " #######################################################\n",
        "\n",
        "print(dataset.head())\n",
        " \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Bvj6ORXPlA5j",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "16fbfcbf-6d11-4c0f-a8e5-f012baea251f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        " ####################################################### \n",
        " # plot Positive Vs. Negative reviews count            # \n",
        " #######################################################\n",
        "\n",
        "sns.countplot(x=dataset['sentiment'])\n",
        "\n",
        "plt.title(\"Positive Vs. Negative reviews count\", fontsize = 15)\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45dT7_ld3aAg"
      },
      "source": [
        "### Cleaning the Reviews (2 Points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "HEHFIPM83Zbz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65997585-98b7-44d0-be3e-8ccfac1b0b4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('words')\n",
        "\n",
        "stopword = nltk.corpus.stopwords.words('english')\n",
        "wn = nltk.WordNetLemmatizer()\n",
        "ps = nltk.PorterStemmer()\n",
        "words = set(nltk.corpus.words.words())\n",
        "\n",
        "# From the first assignment\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"http\\S+\", \"\", text)\n",
        "    text = re.sub(r\"www.\\S+\", \"\", text)\n",
        "    text_links_removed = \"\".join([char for char in text if char not in string.punctuation])\n",
        "    text_cleaned = \" \".join([word for word in re.split('\\W+', text_links_removed)\n",
        "        if word not in stopword])\n",
        "    text = \" \".join([wn.lemmatize(word) for word in re.split('\\W+', text_cleaned)])\n",
        "    text = \"   ayy\"\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "io8EvrJJ3eGu"
      },
      "outputs": [],
      "source": [
        " #############################################################################################  \n",
        " # Clean all the reviews in the dataset using the clean_text function provided above         # \n",
        " ############################################################################################# \n",
        "\n",
        "for review in dataset['review']:\n",
        "    review = clean_text(review)\n",
        " \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4i0C715m3DwZ",
        "outputId": "0019af25-4729-4e32-b5b7-3801c08341ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                              review sentiment\n",
            "0  One of the other reviewers has mentioned that ...  positive\n",
            "1  A wonderful little production. <br /><br />The...  positive\n",
            "2  I thought this was a wonderful way to spend ti...  positive\n",
            "3  Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
            "4  Probably my all-time favorite movie, a story o...  positive\n"
          ]
        }
      ],
      "source": [
        " #####################################################################\n",
        " # print head of the \"CLEANED\" data frame with help of head function # \n",
        " #####################################################################\n",
        "\n",
        "print(dataset.head())\n",
        " \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-TmC4Ebo0Znv"
      },
      "source": [
        "### Splitting the dataset and Encoding Labels (2 Points)\n",
        "\n",
        "Spliting data: <br> \n",
        "80% for the training and the remaining 20% for validation.\n",
        "\n",
        "Encoding Labels: <br>\n",
        "Encode labels as negative and positive as 0 and 1 respectively\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "-n-xz8BMYZE7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52afc49b-c383-4b51-eeb9-f54bb931acab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15525    This movie is a quite fair adaptation of the P...\n",
            "7714     Yeh, I know -- you're quivering with excitemen...\n",
            "5140     The biggest heroes, is one of the greatest mov...\n",
            "10544    I first saw this one afternoon in the 80's on ...\n",
            "20260    I just finished \"Dark Chamber\" aka \"Under Surv...\n",
            "Name: review, dtype: object\n"
          ]
        }
      ],
      "source": [
        "#################################################################################\n",
        "# Split the data using the sklearn module                                       #\n",
        "# 80% for the training and the remaining 20% for validation                     #\n",
        "#################################################################################\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(dataset['review'], dataset['sentiment'], test_size=.2, train_size=.8)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "A6y-74sT-9s1"
      },
      "outputs": [],
      "source": [
        "#################################################################################\n",
        "# Initialize label encoder from sklearn module                                  #\n",
        "# fit on train labels and transform both train and validation labels            #\n",
        "#################################################################################\n",
        "le = LabelEncoder()\n",
        "le.fit(y_train)\n",
        "\n",
        "y_transform = le.transform(y_train)\n",
        "y_transform = le.transform(y_test)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xePIfNU8YZFA"
      },
      "source": [
        "### Pre-Processing The Text (5 Points)\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "TvpwLHnRYZFC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "e9a81900-eefb-4bee-e7ee-0b59edf737af"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-1a32b4916a88>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mvocabulary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'keys'"
          ]
        }
      ],
      "source": [
        "# You can use the modules given below\n",
        "#from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "#from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "###############################################################################  \n",
        "# Fit your tokenizer on the training reviews                                  #\n",
        "###############################################################################\n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#################################################################################################\n",
        "# The word_index dictionary assigns a unique index to each unique word present in the training  #\n",
        "# reviews.                                                                                      #\n",
        "#                                                                                               #\n",
        "# Create the word_index dictionary using the tokenizer                                          #\n",
        "# Find the vocabulary of your training reviews                                                  #\n",
        "#################################################################################################\n",
        "\n",
        "word_index = dict()\n",
        "word_index = tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "\n",
        "vocabulary = word_index.keys()\n",
        "\n",
        "\n",
        "#################################################################################################\n",
        "# Convert the reviews in the dataset to their index form by using a function available          #\n",
        "# with the tokenizer                                                                            #\n",
        "# HINT : convert training and validation reviews into sequences                                 #\n",
        "#################################################################################################\n",
        "index_form_train = tokenizer.texts_to_sequences(X_train)\n",
        "index_form_test = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "\n",
        "#################################################################################################\n",
        "# Pad the the training and validation sequences so all of them have the same length             # \n",
        "# set maxlen = 200                                                                              #\n",
        "#################################################################################################\n",
        "\n",
        "X_train = pad_sequences(index_form_train, maxlen=200)\n",
        "X_test = pad_sequences(index_form_test, maxlen=200)\n",
        "\n",
        "# maxlen which is the maximum length of one review we will use for our training\n",
        "\n",
        "print('Vocabulary : {}'.format(vocabulary))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-yGOMB7YZFJ"
      },
      "source": [
        "### Using glove vectors for embedding (5 Points)\n",
        "\n",
        "GloVe vectors capture both global statistics and local statistics of a corpus. We use GloVe to convert words to embeddings in the vector space based on their semantics. \n",
        "\n",
        "We will be using the 200-dimensional GloVe vectors for the task at hand.\n",
        "\n",
        "To learn more about GloVe please read the following resource:\n",
        "- https://nlp.stanford.edu/pubs/glove.pdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1wjY4_mOYZFP"
      },
      "outputs": [],
      "source": [
        "glove_dictionary = {}\n",
        "with open('glove.6B.200d.txt') as file:\n",
        "    for each_line in file:\n",
        "        words_in_line, coeff_cients = each_line.split(maxsplit=1)\n",
        "        coeff_cients = np.array(coeff_cients.split(),dtype = float)\n",
        "        glove_dictionary[words_in_line] = coeff_cients"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pDYNi0CsYZFR"
      },
      "outputs": [],
      "source": [
        " # All the words which are not in the GloVe dictionary will be assigned a zero vector.\n",
        "\n",
        "embedding_matrix = np.zeros((vocabulary, 200))\n",
        "\n",
        "######################################################################\n",
        "# The glove_dictionary contains words vs their respective embeddings #\n",
        "#                                                                    #\n",
        "# Create the embedding matrix using the glove_dictionary             #\n",
        "######################################################################\n",
        "\n",
        "for i, word in enumerate(vocabulary):\n",
        "    embedding_vector = glove_dictionary[word]\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aXXvMdZ6gJpI"
      },
      "outputs": [],
      "source": [
        "embedding_matrix.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6d-frq1QxJW"
      },
      "source": [
        "Sample output : (99987, 200)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9Fxf0zuYZFZ"
      },
      "source": [
        "### Creating The Model (10)\n",
        "\n",
        "If you need a refresher or have never worked with Neural Networks before, here are a few resources:\n",
        "- https://web.stanford.edu/~jurafsky/slp3/7.pdf\n",
        "- https://web.stanford.edu/~jurafsky/slp3/9.pdf\n",
        "- https://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
        "\n",
        "\n",
        "Training a neural network model will take time. \n",
        "- You can use Google Colab / Kaggle notebooks. You get a free GPU for a limited time.\n",
        "- Without a GPU, You might have to wait longer to experiment.\n",
        "\n",
        "\n",
        "Useful resources : <br>\n",
        "https://www.tensorflow.org/api_docs/python/tf/keras/Sequential <br>\n",
        "https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense <br>\n",
        "https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout <br>\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y8CXAEvwIHHT"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "######################################################\n",
        "# Complete this linear model in tensorflow           #\n",
        "######################################################\n",
        "\n",
        "def build_model(embedding_matrix):\n",
        "\n",
        "  ''' \n",
        "  Arguments:\n",
        "    embedding_matrix : a matrix with the corresponding embeddings\n",
        "    of all words.\n",
        "\n",
        "  Returns:\n",
        "    The LSTM model that you created.\n",
        "  '''\n",
        "  \n",
        "  model = tf.keras.Sequential()\n",
        "\n",
        "  # TO DO: layer 1 : add embedding layer\n",
        "  # The embedding layer maps the words to their embedding vectors from the embedding matrix\n",
        "  model.add(tf.keras.layers.Embedding(vocabulary, 200, weights=[embedding_matrix], input_length=maxlen, trainable=False))\n",
        "\n",
        "\n",
        "\n",
        "  # TO DO: layer 2 : add Bidirectional LSTM Layer \n",
        "  model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64), input_shape=(maxlen,)))\n",
        "\n",
        "\n",
        "  # TO DO Add more layers : you can add more dense layers and dropout\n",
        "  # NOTE : You should be able to achieve an validation accuracy greater than 85%\n",
        "  #        within 10 epochs\n",
        " # layer 2 : add the dense layer with 64 units and relu activation\n",
        "  model.add(tf.keras.layers.Dense(units=64, activation=tf.nn.relu))\n",
        "\n",
        "  # layer 3 : add the dropout layer with dropout rate of 0.5\n",
        "  model.add(tf.keras.layers.Dropout(rate=0.5))\n",
        "  \n",
        "  # layer 4 : add the dense layer with 32 units with tanh activation and with l2 regularization\n",
        "  model.add(tf.keras.layers.Dense(units=32, activation=tf.nn.tanh, kernel_regularizer='l2')) \n",
        "\n",
        "  # layer 5 : add the dropout layer with dropout rate of 0.5\n",
        "  model.add(tf.keras.layers.Dropout(rate=0.5))\n",
        "\n",
        "  # layer 6 : add the dense layer with 16 units with tanh activation and with l2 regularization\n",
        "  model.add(tf.keras.layers.Dense(units=16, activation=tf.nn.tanh, kernel_regularizer='l2')) \n",
        "\n",
        "  # layer 7 : add the dropout layer with dropout rate of 0.5\n",
        "  model.add(tf.keras.layers.Dropout(rate=0.5))\n",
        "\n",
        "\n",
        "  # TO DO Final layer : add output layer and activation \n",
        "  model.add(tf.keras.layers.Dense(1, activation=tf.nn.sigmoid))\n",
        "\n",
        "\n",
        "  # TO DO : use a loss function, optimizer as adam to compile \n",
        "  # and evaluate model on auc,precision,recall,accuracy\n",
        "  # HINT : choose your loss function based on the task (binary classification)\n",
        "  model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['AUC'])\n",
        "\n",
        "  return model\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dxl8HDkHYZFc"
      },
      "outputs": [],
      "source": [
        "# NOTE : You should be able to achieve an validation accuracy greater than 85%\n",
        "#        within 10 epochs\n",
        "\n",
        "#################################################################\n",
        "# Call the build_model function and initialize the model        #\n",
        "#################################################################\n",
        "model = build_model(X_train)\n",
        "  \n",
        "\n",
        "#######################################################################################################\n",
        "# train and validate the model on the padded sequences of text which we have created initially        #\n",
        "#######################################################################################################\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import roc_curve\n",
        "\n",
        "skf = KFold(n_splits=5)\n",
        "\n",
        "# Now Split the countvectors and target (y)\n",
        "split_X = skf.split(X)\n",
        "\n",
        "\n",
        "# iterate through the train and valid index in splits for 5 folds\n",
        "for  i, (train_index, test_index) in enumerate(split_X):\n",
        "  # Get X_train, X_valid, y_train, y_valid using indexes\n",
        "\n",
        "  X_train, X_valid = X[train_index], X[test_index] \n",
        "  y_train, y_valid = y_train.iloc[train_index], y_test.iloc[test_index]\n",
        "  \n",
        "  #call the build_model function and initialize the model\n",
        "  X_train = X_train.toarray()\n",
        "  X_valid = X_valid.toarray()\n",
        "  y_train = np.asarray(y_train)\n",
        "  y_valid = np.asarray(y_valid)\n",
        "\n",
        "  \n",
        "  # train and validate the model on the count vectors of text which we have created initially for 5 epochs, \n",
        "  # adjust batch size according to your computation power (suggestion use : 16)\n",
        "  history = model.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    batch_size=15,\n",
        "    epochs=5,\n",
        "    validation_data=(X_valid, y_valid)\n",
        ")\n",
        "\n",
        "  preds = model.predict(X_valid)\n",
        "\n",
        "  fig, ax = plt.subplots( figsize=(10,8))\n",
        "\n",
        "  # plot the graph between training auc and validation auc\n",
        "  fpr, tpr, threshold = roc_curve(\n",
        "      y_true=y_valid, y_score = preds)\n",
        "      \n",
        " \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyvT7vTmYZFd"
      },
      "source": [
        "### Plotting Accuracy and Losses (5 Points)\n",
        " \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uCXJzXMjYZFe"
      },
      "outputs": [],
      "source": [
        "####################################\n",
        "# PLOT :                           #\n",
        "# train loss vs val loss           #\n",
        "# train auc vs val auc             #\n",
        "# train recall vs val recall       #\n",
        "# train precision vs val precision #\n",
        "# train accuracy vs val accuracy   #\n",
        "####################################\n",
        "\n",
        " \n",
        "plt.plot(fpr, tpr, color='red')\n",
        "plt.title('Receiver Operating Characteristic Curve', size=20)\n",
        "plt.plot([0, 1], [0, 1], color='green', linestyle=':')\n",
        "plt.xlabel('False Positive Rate', size=15)\n",
        "plt.ylabel('True Positive Rate', size=15)\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "me9so1i_ecsA"
      },
      "outputs": [],
      "source": [
        "# You will need to include the pickled model along with the other submission files\n",
        "# The saved model will be used to verify your lstm's predictions on hidden reviews\n",
        "\n",
        "##################################################################################\n",
        "# Save your trained model as a pickle file named \"lstm_classifier\"               #\n",
        "# You will be using this saved model to make predictions in the next module      #\n",
        "##################################################################################\n",
        "\n",
        "import pickle \n",
        "\n",
        "with open(\"lstm_classifier.pkl\", \"wb\") as f:\n",
        "    pickle.dump(model, f, protocol=pickle.HIGHEST_PROTOCOL)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5zTQmf4Xr_O"
      },
      "source": [
        "### Prediction (5 Points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PRalffaWeLxN"
      },
      "outputs": [],
      "source": [
        "######################################################################\n",
        "# Load your saved model                                              #\n",
        "# Use the saved model to make predictions                            #\n",
        "######################################################################\n",
        "\n",
        "with open(\"lstm_classifier.pkl\", \"rb\") as f:\n",
        "    saved_model = pickle.load(f)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yM3JAQdPXq-y"
      },
      "outputs": [],
      "source": [
        "# use the saved model to predict the reviews\n",
        "def predict_review(review):\n",
        "  '''\n",
        "  Arguments:\n",
        "    review : A single review for which you want to predict the sentiment for.\n",
        "             example: \"This movie was amazing! I would defenitely watch it again.\"\n",
        "\n",
        "  Returns:\n",
        "    The predicted sentiment for the review : either 1 or 0\n",
        "  '''\n",
        "\n",
        "\n",
        " ############################################################################# \n",
        " # Predict the sentiment for the given review using the model                #\n",
        " # that you trained and return the sentiment                                 #\n",
        " #                                                                           #\n",
        " # HINT : Remember that the review needs to be \"preprocessed\" before you use #\n",
        " # it for prediction                                                         #\n",
        " #############################################################################\n",
        "  review = preprocess(review)\n",
        "  preds = model.predict(review)\n",
        "\n",
        "  return preds\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BM0RnmVOdckJ"
      },
      "outputs": [],
      "source": [
        "# Do not edit this cell\n",
        "\n",
        "for review in [\"If you like original gut wrenching laughter you will like this movie. If you are young or old then you will love this movie, hell even my mom liked it.<br /><br />Great Camp!!!\",\n",
        "                \"What a waste of talent. A very poor, semi-coherent, script cripples this film. Rather unimaginative direction, too. Some VERY faint echoes of Fargo here, but it just doesn't come off.\",\n",
        "                \"I have seen this film at least 100 times and I am still excited by it, the acting is perfect and the romance between Joe and Jean keeps me on the edge of my seat, plus I still think Bryan Brown is the tops. Brilliant Film.\",\n",
        "                \"Cheap, amateurish, unimaginative, exploitative... but don't think it'll have redeeming amusement value. About as unentertaining, uninstructive and just plain dull as a film can be.\"]:\n",
        "    p = predict_review(review)\n",
        "    print(f'{review[:100]} -> {p}')\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6cxqkVn41Kj"
      },
      "source": [
        "### Expected Output:\n",
        "\n",
        "If you like original gut wrenching laughter you will like this movie. If you are young or old then y -> 1 <br>\n",
        "What a waste of talent. A very poor, semi-coherent, script cripples this film. Rather unimaginative  -> 0 <br>\n",
        "I have seen this film at least 100 times and I am still excited by it, the acting is perfect and the -> 1 <br>\n",
        "Cheap, amateurish, unimaginative, exploitative... but don't think it'll have redeeming amusement val -> 0"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}